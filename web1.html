<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Interactive Report: Saliency-Aware AI</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;900&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Subtle & Sophisticated (#F5F5F5, #E8E8E8, #333333, #4A90E2) -->
    <!-- 
      Application Structure Plan: A narrative, single-page application designed to guide the user from problem to solution.
      1.  **Hero/Intro:** Hook the user by introducing the "black box" problem.
      2.  **Interactive Architecture:** Present the model's components as interactive cards. Hovering reveals details, promoting exploration over passive viewing. This structure is chosen to break down a complex system into digestible, user-driven pieces.
      3.  **Performance Dashboard:** Combine a primary bar chart for high-level comparison with a clickable interaction that reveals deeper context about each dataset. This tiered information approach prevents overwhelming the user while allowing for deeper dives.
      4.  **Saliency Demo:** The core "wow" factor. A direct, interactive simulation where the user clicks a button to see the saliency concept in action. This is the most effective way to communicate the main innovation.
      5.  **Conclusion:** A clear summary of the project's value.
      This user flow transforms a static report into an engaging learning tool.
    -->
    <!-- 
      Visualization & Content Choices:
      - **The Challenge:** Goal: Inform. Method: Large typographic icon (❓) and text. Justification: Strong, immediate visual hook for the core problem.
      - **Architecture Diagram:** Goal: Organize/Inform. Method: Interactive cards using HTML/CSS/JS. Interaction: Hover to show details. Justification: More engaging than a static flowchart; encourages user interaction to learn. Library: Vanilla JS.
      - **Performance Chart:** Goal: Compare/Inform. Method: Interactive Bar Chart. Interaction: Click bar to update a separate text area with context. Justification: Bar charts are excellent for comparison. The click interaction adds a layer of depth and discovery. Library: Chart.js (Canvas).
      - **Saliency Demo:** Goal: Inform/Engage. Method: Side-by-side comparison with a button-triggered visual effect (CSS gradient overlay). Justification: The most direct way to demonstrate the abstract concept of saliency, providing a clear "aha!" moment. Library: Vanilla JS.
      - CONFIRMATION: NO SVG graphics used. NO Mermaid JS used.
    -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F5F5F5;
            color: #333333;
        }
        .accent-color { color: #4A90E2; }
        .accent-bg { background-color: #4A90E2; }
        .accent-border { border-color: #4A90E2; }
        .card {
            background-color: #FFFFFF;
            border-radius: 0.75rem;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1);
            transition: all 0.3s ease-in-out;
        }
        .nav-link {
            transition: color 0.3s ease;
        }
        .nav-link:hover {
            color: #4A90E2;
        }
        .chart-container {
            position: relative;
            width: 100%;
            height: 28rem;
            max-height: 450px;
        }
        .arch-card {
            border: 1px solid #E8E8E8;
            cursor: pointer;
        }
        .arch-card:hover {
            border-color: #4A90E2;
            transform: translateY(-4px);
            box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -2px rgb(0 0 0 / 0.1);
        }
        .saliency-overlay {
            position: absolute;
            inset: 0;
            opacity: 0;
            transition: opacity 0.5s ease-in-out;
            background: radial-gradient(circle at 60% 35%, rgba(255, 235, 59, 0.8) 5%, rgba(255, 152, 0, 0.6) 20%, rgba(244, 67, 54, 0.2) 45%, rgba(244, 67, 54, 0) 70%);
            border-radius: 0.5rem;
        }
        .saliency-overlay.visible {
            opacity: 1;
        }
    </style>
</head>
<body class="antialiased">

    <nav class="bg-white/80 backdrop-blur-lg sticky top-0 z-50 shadow-sm">
        <div class="container mx-auto px-6 py-3 flex justify-between items-center">
            <h1 class="text-xl font-bold">Saliency-Aware AI Explorer</h1>
            <div class="hidden md:flex space-x-8">
                <a href="#problem" class="nav-link font-medium">The Problem</a>
                <a href="#architecture" class="nav-link font-medium">Architecture</a>
                <a href="#performance" class="nav-link font-medium">Performance</a>
                <a href="#demo" class="nav-link font-medium">Interactive Demo</a>
            </div>
        </div>
    </nav>

    <main class="container mx-auto p-4 md:p-8">

        <section id="problem" class="min-h-[60vh] flex items-center text-center py-16">
            <div class="max-w-4xl mx-auto">
                <h2 class="text-5xl md:text-7xl font-black mb-4">What's inside the <span class="accent-color">Black Box</span>?</h2>
                <p class="text-lg md:text-xl text-gray-600 leading-relaxed">
                    Convolutional Neural Networks (CNNs) are powerful tools in science and industry, but their internal logic is often a mystery. In critical fields like medicine and autonomous driving, we need more than just accurate predictions—we need to understand *why* a decision was made. This project tackles that challenge head-on by building a model that is both powerful and transparent.
                </p>
            </div>
        </section>

        <section id="architecture" class="py-16">
            <div class="text-center max-w-3xl mx-auto mb-12">
                <h2 class="text-4xl font-bold mb-4">Anatomy of an Interpretable AI</h2>
                <p class="text-lg text-gray-600">
                    Our framework is built on a sophisticated architecture that balances feature extraction, classification, and explainability. Hover over each component to understand its specific role in making the AI's decisions transparent.
                </p>
            </div>
            <div class="grid grid-cols-1 md:grid-cols-3 gap-8 mb-8">
                <div class="arch-card card p-6" data-info="input-info">
                    <h3 class="text-2xl font-bold mb-2 accent-color">1. Encoder</h3>
                    <p class="font-semibold">Pre-trained ResNet-18</p>
                    <p class="text-gray-600 mt-2">Leverages a powerful, pre-trained network to perform robust feature extraction, identifying key patterns in the input image.</p>
                </div>
                <div class="arch-card card p-6" data-info="processing-info">
                    <h3 class="text-2xl font-bold mb-2 accent-color">2. Core Processors</h3>
                     <p class="font-semibold">Decoder & Classifier</p>
                    <p class="text-gray-600 mt-2">A custom decoder reconstructs the input image while a specialized head performs the final classification, predicting the digit's label.</p>
                </div>
                <div class="arch-card card p-6" data-info="output-info">
                    <h3 class="text-2xl font-bold mb-2 accent-color">3. The Explainer</h3>
                     <p class="font-semibold">Fixation Point Generator (FPG)</p>
                    <p class="text-gray-600 mt-2">This is the key to interpretability. The FPG generates a "saliency map" that highlights the exact regions the model focused on.</p>
                </div>
            </div>
            <div id="architecture-info-panel" class="card p-6 min-h-[100px] bg-gray-50 border-2 border-dashed border-gray-300 transition-all duration-300">
                <p class="text-gray-700 font-medium text-center">Hover over a component to see more details here.</p>
            </div>
        </section>

        <section id="performance" class="py-16">
            <div class="text-center max-w-3xl mx-auto mb-12">
                <h2 class="text-4xl font-bold mb-4">Performance Meets Precision</h2>
                <p class="text-lg text-gray-600">
                    A transparent model is only useful if it's also highly accurate. Our framework excels, demonstrating state-of-the-art performance across diverse and challenging handwritten digit datasets. Click on a bar in the chart to learn more about the dataset it represents.
                </p>
            </div>
            <div class="grid grid-cols-1 lg:grid-cols-5 gap-8 items-center">
                <div class="lg:col-span-3 card p-6">
                    <div class="chart-container">
                        <canvas id="accuracyChart"></canvas>
                    </div>
                </div>
                <div id="dataset-info-panel" class="lg:col-span-2 card p-8 min-h-[200px] bg-gray-50 border-2 border-dashed border-gray-300 flex items-center justify-center transition-all duration-300">
                     <p class="text-gray-700 font-medium text-center">Click a bar to see dataset details.</p>
                </div>
            </div>
        </section>

        <section id="demo" class="py-16">
            <div class="text-center max-w-3xl mx-auto mb-12">
                <h2 class="text-4xl font-bold mb-4">Saliency in Action: An Interactive Demo</h2>
                 <p class="text-lg text-gray-600">
                    This is where transparency becomes tangible. The "Fixation Point Generator" produces a saliency map—a heat map showing where the AI is "looking." Click the button below to see how our model focuses on the key strokes of the digit '8' to make its prediction.
                </p>
            </div>
            <div class="card p-8">
                <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                    <div class="text-center">
                        <div class="relative w-64 h-64 bg-gray-200 mx-auto rounded-lg flex items-center justify-center">
                            <span class="text-9xl font-black text-gray-800">8</span>
                            <div id="saliency-map" class="saliency-overlay"></div>
                        </div>
                    </div>
                    <div class="text-center md:text-left">
                        <h3 class="text-2xl font-bold mb-4">Prediction: <span class="accent-color">8</span></h3>
                        <p class="text-gray-600 mb-6">Our model confidently identifies the digit. But how? The saliency map reveals its focus on the overlapping loops characteristic of the number 8, ignoring the empty space.</p>
                        <button id="toggleSaliencyBtn" class="accent-bg text-white font-bold py-3 px-6 rounded-lg shadow-lg hover:opacity-90 transition-opacity">
                            Reveal AI's Focus
                        </button>
                    </div>
                </div>
            </div>
        </section>

    </main>
    
    <footer class="text-center py-8 mt-12 border-t border-gray-200">
        <p class="text-gray-600">A framework for building more transparent and trustworthy AI systems.</p>
    </footer>

    <script>
    document.addEventListener('DOMContentLoaded', () => {

        const architectureData = {
            'input-info': {
                title: 'Encoder (ResNet-18)',
                text: 'The process begins by feeding the handwritten digit image into a pre-trained ResNet-18 model. This powerful encoder acts as a sophisticated feature extractor, converting the raw pixel data into a rich, high-level representation that captures essential patterns like curves, lines, and edges.'
            },
            'processing-info': {
                title: 'Decoder & Classification Head',
                text: 'The extracted features are then processed by two parallel components. A custom decoder works to reconstruct the original image from the features, ensuring the model retains a complete understanding. Simultaneously, a classification head analyzes the features to make the final prediction of the digit\'s identity.'
            },
            'output-info': {
                title: 'Fixation Point Generator (FPG)',
                text: 'This is the core of our model\'s interpretability. The FPG analyzes the high-level features to predict a spatial attention map, or "saliency map." This map highlights the specific regions of the input image that were most influential in the classification decision, effectively showing us what the model "looked at."'
            }
        };

        const archCards = document.querySelectorAll('.arch-card');
        const archInfoPanel = document.getElementById('architecture-info-panel');
        const defaultArchText = archInfoPanel.innerHTML;

        archCards.forEach(card => {
            card.addEventListener('mouseenter', () => {
                const infoKey = card.dataset.info;
                const info = architectureData[infoKey];
                archInfoPanel.innerHTML = `<h4 class="text-xl font-bold accent-color mb-2">${info.title}</h4><p class="text-gray-700">${info.text}</p>`;
                archInfoPanel.classList.remove('bg-gray-50', 'border-dashed', 'border-gray-300');
                archInfoPanel.classList.add('bg-blue-50', 'border-solid', 'accent-border');
            });
            card.addEventListener('mouseleave', () => {
                archInfoPanel.innerHTML = defaultArchText;
                 archInfoPanel.classList.add('bg-gray-50', 'border-dashed', 'border-gray-300');
                archInfoPanel.classList.remove('bg-blue-50', 'border-solid', 'accent-border');
            });
        });

        const datasetInfoData = {
            'MNIST Dataset': {
                text: 'A foundational benchmark in machine learning, the MNIST dataset consists of 70,000 small, grayscale images of handwritten digits (0-9). Its uniformity makes it ideal for testing baseline model performance.'
            },
            'Malayalam Handwritten Dataset': {
                text: 'This dataset presents a greater challenge due to the complex, cursive nature of Malayalam script. High accuracy here demonstrates the model\'s ability to handle intricate and varied character forms.'
            },
            'English Handwritten Dataset': {
                text: 'While familiar, handwritten English digits exhibit a wide variety of styles. Success on this dataset shows the model\'s robustness and ability to generalize across different handwriting.'
            }
        };

        const datasetInfoPanel = document.getElementById('dataset-info-panel');
        const defaultDatasetText = datasetInfoPanel.innerHTML;
        
        function wrapLabel(str, maxWidth) {
            if (!str) return '';
            if (str.length <= maxWidth) return str;
            const words = str.split(' ');
            let lines = [];
            let currentLine = '';
            words.forEach(word => {
                if ((currentLine + ' ' + word).trim().length > maxWidth && currentLine.length > 0) {
                    lines.push(currentLine);
                    currentLine = word;
                } else {
                    currentLine = (currentLine + ' ' + word).trim();
                }
            });
            if (currentLine.length > 0) {
                lines.push(currentLine);
            }
            return lines;
        }
        
        const accuracyData = {
            labels: ['MNIST Dataset', 'Malayalam Handwritten Dataset', 'English Handwritten Dataset'],
            scores: [99.56, 98.44, 97.81]
        };
        const processedLabels = accuracyData.labels.map(label => wrapLabel(label, 16));
        
        const ctx = document.getElementById('accuracyChart').getContext('2d');
        const accuracyChart = new Chart(ctx, {
            type: 'bar',
            data: {
                labels: processedLabels,
                datasets: [{
                    label: 'Accuracy',
                    data: accuracyData.scores,
                    backgroundColor: ['#4A90E2', '#63a3e8', '#89bced'],
                    borderColor: '#336cb2',
                    borderWidth: 1,
                    borderRadius: 4,
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                indexAxis: 'y',
                scales: {
                    x: {
                        beginAtZero: false,
                        min: 97,
                        max: 100,
                        title: {
                            display: true,
                            text: 'Classification Accuracy (%)',
                            font: { size: 14 }
                        }
                    },
                    y: {
                        ticks: {
                            font: { size: 14 }
                        }
                    }
                },
                plugins: {
                    legend: {
                        display: false
                    },
                    tooltip: {
                        callbacks: {
                            title: function(tooltipItems) {
                                const item = tooltipItems[0];
                                let label = item.chart.data.labels[item.dataIndex];
                                return Array.isArray(label) ? label.join(' ') : label;
                            }
                        }
                    }
                },
                onClick: (event, elements) => {
                    if (elements.length > 0) {
                        const elementIndex = elements[0].index;
                        const label = accuracyChart.data.labels[elementIndex];
                        const fullLabel = Array.isArray(label) ? label.join(' ') : label;
                        const info = datasetInfoData[fullLabel];

                        if (info) {
                            datasetInfoPanel.innerHTML = `<h4 class="text-xl font-bold accent-color mb-2">${fullLabel}</h4><p class="text-gray-700">${info.text}</p>`;
                            datasetInfoPanel.classList.remove('bg-gray-50', 'border-dashed', 'border-gray-300');
                            datasetInfoPanel.classList.add('bg-blue-50', 'border-solid', 'accent-border');
                        }
                    } else {
                        datasetInfoPanel.innerHTML = defaultDatasetText;
                        datasetInfoPanel.classList.add('bg-gray-50', 'border-dashed', 'border-gray-300');
                        datasetInfoPanel.classList.remove('bg-blue-50', 'border-solid', 'accent-border');
                    }
                }
            }
        });

        const toggleBtn = document.getElementById('toggleSaliencyBtn');
        const saliencyMap = document.getElementById('saliency-map');
        toggleBtn.addEventListener('click', () => {
            const isVisible = saliencyMap.classList.toggle('visible');
            toggleBtn.textContent = isVisible ? "Hide AI's Focus" : "Reveal AI's Focus";
        });
    });
    </script>
</body>
</html>
